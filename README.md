# fftax ğŸŒŠ

**Non-Uniform FFTs that actually play nice with JAX.** âœ¨

[![CI](https://github.com/geoffroyO/nufftax/actions/workflows/ci.yml/badge.svg)](https://github.com/geoffroyO/nufftax/actions/workflows/ci.yml)
[![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

---

Ever tried to `jax.grad` through a NUFFT and got hit with a wall of errors? ğŸ˜¤ Yeah, us too.

**fftax** is a pure JAX implementation of the Non-Uniform Fast Fourier Transform. No C++ bindings, no XLA custom calls, no tears. Just JAX all the way down. ğŸ

## âœ¨ Features

- ğŸ§¬ **Pure JAX** - Works with `jit`, `grad`, `vmap`, `jvp`, `vjp`... the whole gang
- ğŸ“ **1D, 2D, 3D** - Type 1 (nonuniform â†’ uniform) and Type 2 (uniform â†’ nonuniform)
- ğŸ¯ **Differentiable** - Gradients with respect to both points and values
- ğŸš€ **GPU-ready** - Runs on CPU and GPU without code changes

## ğŸ“¦ Installation

```bash
pip install fftax
```

Or from source:

```bash
git clone https://github.com/geoffroyO/nufftax.git
cd nufftax
pip install -e ".[dev]"
```

## ğŸ Quick Start

```python
import jax.numpy as jnp
from fftax import nufft1d1, nufft1d2

# Some nonuniform points in [0, 2Ï€)
x = jnp.array([0.1, 0.5, 1.0, 2.0, 4.5])
c = jnp.array([1+1j, 2-1j, 0.5, 1j, -1+0.5j])

# Type 1: Nonuniform points â†’ Fourier modes
f = nufft1d1(x, c, n_modes=64, eps=1e-6)

# Type 2: Fourier modes â†’ Nonuniform points
c_back = nufft1d2(x, f, eps=1e-6)
```

## ğŸ‰ The Fun Part: Autodiff Just Works

```python
import jax

# Gradient w.r.t. the strengths
def loss(c):
    f = nufft1d1(x, c, n_modes=64, eps=1e-6)
    return jnp.sum(jnp.abs(f) ** 2)

grad_c = jax.grad(loss)(c)  # It just works! ğŸŠ

# Gradient w.r.t. the points (yes, really)
def loss_x(x):
    f = nufft1d1(x, c, n_modes=64, eps=1e-6)
    return jnp.sum(jnp.abs(f) ** 2)

grad_x = jax.grad(loss_x)(x)  # This too! ğŸ¤¯

# Batch over multiple sets of points
batched_nufft = jax.vmap(lambda xi: nufft1d1(xi, c, n_modes=64))
x_batch = jnp.stack([x, x + 0.1, x + 0.2])
f_batch = batched_nufft(x_batch)  # Shape: (3, 64) ğŸ“¦
```

## ğŸ“š API Reference

### Type 1: Nonuniform â†’ Uniform ğŸ”€

```python
# 1D
f = nufft1d1(x, c, n_modes, eps=1e-6, iflag=1)

# 2D
f = nufft2d1(x, y, c, n_modes, eps=1e-6, iflag=1)

# 3D
f = nufft3d1(x, y, z, c, n_modes, eps=1e-6, iflag=1)
```

Computes: `f[k] = Î£â±¼ c[j] Â· exp(i Â· iflag Â· k Â· x[j])`

### Type 2: Uniform â†’ Nonuniform ğŸ”„

```python
# 1D
c = nufft1d2(x, f, eps=1e-6, iflag=1)

# 2D
c = nufft2d2(x, y, f, eps=1e-6, iflag=1)

# 3D
c = nufft3d2(x, y, z, f, eps=1e-6, iflag=1)
```

Computes: `c[j] = Î£â‚– f[k] Â· exp(i Â· iflag Â· k Â· x[j])`

### Parameters ğŸ›ï¸

| Parameter | Description |
|-----------|-------------|
| `x, y, z` | Nonuniform points in [0, 2Ï€) |
| `c` | Complex strengths at nonuniform points |
| `f` | Fourier mode coefficients |
| `n_modes` | Number of output modes (int or tuple) |
| `eps` | Requested precision (1e-2 to 1e-14) |
| `iflag` | Sign of exponent: +1 or -1 |

## ğŸ”§ How It Works

fftax implements the NUFFT using the standard approach:

1. ğŸ“¡ **Spreading/Interpolation** - Convolve with a compactly-supported kernel (exponential of semicircle)
2. ğŸ”¢ **FFT** - Standard FFT on the oversampled grid
3. â— **Deconvolution** - Divide by kernel Fourier coefficients

The magic is that everything is written in pure JAX, so autodiff propagates through naturally. âœ¨

## âš¡ Performance

It's fast enough for most research purposes. For production workloads with massive transforms, you might want [finufft](https://github.com/flatironinstitute/finufft) - but then you lose the autodiff superpowers. ğŸ¦¸

## ğŸ§ª Running Tests

```bash
# Install dev dependencies
pip install -e ".[dev]"

# Run tests
pytest tests/ -v

# With coverage
pytest tests/ -v --cov=fftax
```

## ğŸ“„ License

MIT

## ğŸ™ Acknowledgments

Algorithm based on the excellent [FINUFFT](https://github.com/flatironinstitute/finufft) library by the Flatiron Institute. ğŸ’™
